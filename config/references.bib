book{book:sample,
  author    = {Ahmad Surahmad and Bedu Pian Sebedu},
  year      = {2020},
  month     = {Januari},
  pages     = {},
  title     = {Buku Test},
  isbn      = {999-9-999-99999-9},
  journal   = {Judul Jurnal},
  doi       = {99.9999/9-999-99999-9},
  publisher = {Universitas Antah Berantah},
  address   = {Depak}
}

manual{ui:pedoman_ta,
  author    = {{Universitas Indonesia}},
  year      = {2017},
  month     = {Oktober},
  date      = {17},
  title     = {Pedoman Teknis Penulisan Tugas Akhir Mahasiswa Universitas Indonesia},
  url       = {https://lib.ui.ac.id/unduh/unduh/TA_UI.pdf},
  edition   = {Revisi},
  publisher = {Universitas Indonesia},
  address   = {Depok}
}

article{latex:intro,
  author      = {Jeff Clark},
  year        = {2010},
  month       = {Januari},
  date        = {26},
  title       = {Introduction to \latex},
  url         = {http://frodo.elon.edu/tutorial/tutorial/node3.html},
  lastchecked = {2020-09-12}
}

manual{latex:source_code_listings,
  author      = {Carsten Heinz and Brooks Moses and Jobst Hoffmann},
  title       = {The Listings Package},
  year        = {2024},
  month       = {September},
  date        = {23},
  url         = {https://mirror.unpad.ac.id/ctan/macros/latex/contrib/listings/listings.pdf},
  edition     = {1.10c},
  pages       = {15},
  lastchecked = {2024-12-13}
}

@inproceedings{Aristotle25,
  author    = {Jundong Xu and
               Hao Fei and
               Meng Luo and
               Qian Liu and
               Liangming Pan and
               William Yang Wang and
               Preslav Nakov and
               Mong{-}Li Lee and
               Wynne Hsu},
  title     = {Aristotle: Mastering Logical Reasoning with {A} Logic-Complete Decompose-Search-Resolve Framework},
  booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics},
  year      = {2025},
  url       = {https://arxiv.org/abs/2412.16953}
}

@misc{qwen2.5,
  title  = {Qwen2.5: A Party of Foundation Models},
  url    = {https://qwenlm.github.io/blog/qwen2.5/},
  author = {Qwen Team},
  month  = {September},
  year   = {2024}
}

@article{qwen2,
  title   = {Qwen2 Technical Report},
  author  = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
  journal = {arXiv preprint arXiv:2407.10671},
  year    = {2024}
}

@misc{sea_lion_2024,
  title        = {SEA-LION (Southeast Asian Languages In One Network): A Family of Large Language Models for Southeast Asia},
  author       = {AI Singapore},
  year         = {2024},
  howpublished = {\url{https://github.com/aisingapore/sealion}}
}

@misc{2504.05747,
  title         = {SEA-LION: Southeast Asian Languages in One Network},
  author        = {Raymond Ng and Thanh Ngan Nguyen and Yuli Huang and Ngee Chia Tai and Wai Yi Leong and Wei Qi Leong and Xianbin Yong and Jian Gang Ngui and Yosephine Susanto and Nicholas Cheng and Hamsawardhini Rengarajan and Peerat Limkonchotiwat and Adithya Venkatadri Hulagadri and Kok Wai Teng and Yeo Yeow Tong and Bryan Siow and Wei Yi Teo and Wayne Lau and Choon Meng Tan and Brandon Ong and Zhi Hao Ong and Jann Railey Montalan and Adwin Chan and Sajeban Antonyrex and Ren Lee and Esther Choa and David Ong Tat-Wee and Bing Jie Darius Liu and William Chandra Tjhi and Erik Cambria and Leslie Teo},
  year          = {2025},
  eprint        = {2504.05747},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2504.05747}
}

@inproceedings{koto-etal-2023-indommlu,
  title     = {Large Language Models Only Pass Primary School Exams in {I}ndonesia: A Comprehensive Test on {I}ndo{MMLU}},
  author    = {Fajri Koto and Nurul Aisyah and Haonan Li and Timothy Baldwin},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = December,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{saparov2023language,
  title     = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
  author    = {Abulhair Saparov and He He},
  booktitle = {The Eleventh International Conference on Learning Representations },
  year      = {2023},
  url       = {https://openreview.net/forum?id=qFVVBzXxR2V}
}

@misc{cobbe2021trainingverifierssolvemath,
  title         = {Training Verifiers to Solve Math Word Problems},
  author        = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  year          = {2021},
  eprint        = {2110.14168},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2110.14168}
}

@misc{mishra2022numgluesuitefundamentalchallenging,
  title         = {NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks},
  author        = {Swaroop Mishra and Arindam Mitra and Neeraj Varshney and Bhavdeep Sachdeva and Peter Clark and Chitta Baral and Ashwin Kalyan},
  year          = {2022},
  eprint        = {2204.05660},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2204.05660}
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year          = {2023},
  eprint        = {2201.11903},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2201.11903}
}

@article{Yao2023TreeOT,
  title   = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author  = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2305.10601},
  url     = {https://api.semanticscholar.org/CorpusID:258762525}
}

@inproceedings{pan-etal-2023-logic,
  title     = {Logic-{LM}: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning},
  author    = {Pan, Liangming  and
               Albalak, Alon  and
               Wang, Xinyi  and
               Wang, William},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-emnlp.248/},
  doi       = {10.18653/v1/2023.findings-emnlp.248},
  pages     = {3806--3824},
  abstract  = {Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver{'}s error messages to revise symbolic formalizations. We demonstrate Logic-LM{'}s effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2{\%} over using LLM alone with standard prompting and 18.4{\%} over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning.}
}
@misc{xu2024faithfullogicalreasoningsymbolic,
  title         = {Faithful Logical Reasoning via Symbolic Chain-of-Thought},
  author        = {Jundong Xu and Hao Fei and Liangming Pan and Qian Liu and Mong-Li Lee and Wynne Hsu},
  year          = {2024},
  eprint        = {2405.18357},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2405.18357}
}

@book{brachman2004knowledge,
  title     = {Knowledge Representation and Reasoning},
  author    = {Brachman, R. and Levesque, H.},
  isbn      = {9781558609327},
  lccn      = {2004046573},
  series    = {The Morgan Kaufmann Series in Artificial Intelligence},
  url       = {https://books.google.co.id/books?id=OuPtLaA5QjoC},
  year      = {2004},
  publisher = {Elsevier Science}
}

@book{fitting-fol-atp,
  author    = {Melvin Fitting},
  language  = {english},
  title     = {First-Order Logic and Automated Theorem Proving},
  edition   = {2nd},
  publisher = {Springer},
  year      = {1996}
}

@book{huth-ryan-lics2,
  author    = {Michael R. A. Huth and Mark D. Ryan},
  title     = {Logic in Computer Science: Modelling and Reasoning about Systems},
  edition   = {2nd},
  publisher = {Cambridge University Press},
  year      = {2004},
  language  = {english}
}

@inproceedings{tian-etal-2021-diagnosing,
  title     = {Diagnosing the First-Order Logical Reasoning Ability Through {L}ogic{NLI}},
  author    = {Tian, Jidong  and
               Li, Yitian  and
               Chen, Wenqing  and
               Xiao, Liqiang  and
               He, Hao  and
               Jin, Yaohui},
  editor    = {Moens, Marie-Francine  and
               Huang, Xuanjing  and
               Specia, Lucia  and
               Yih, Scott Wen-tau},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.303/},
  doi       = {10.18653/v1/2021.emnlp-main.303},
  pages     = {3738--3747},
  abstract  = {Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability.}
}

@misc{tafjord2021proofwritergeneratingimplicationsproofs,
  title         = {ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language},
  author        = {Oyvind Tafjord and Bhavana Dalvi Mishra and Peter Clark},
  year          = {2021},
  eprint        = {2012.13048},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2012.13048}
}

@inproceedings{NIPS2017_3f5ee243,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@inproceedings{NEURIPS2020_1457c0d6,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{NEURIPS2022_b1efde53,
  author    = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {27730--27744},
  publisher = {Curran Associates, Inc.},
  title     = {Training language models to follow instructions with human feedback},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
  volume    = {35},
  year      = {2022}
}

@inproceedings{10.5555/3600270.3602468,
  author    = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  title     = {LLM.int8(): 8-bit matrix multiplication for transformers at scale},
  year      = {2022},
  isbn      = {9781713871088},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, <b>LLM.int8()</b>. We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open source our software.},
  booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
  articleno = {2198},
  numpages  = {15},
  location  = {New Orleans, LA, USA},
  series    = {NIPS '22}
}

@misc{belcak2025smalllanguagemodelsfuture,
  title         = {Small Language Models are the Future of Agentic AI},
  author        = {Peter Belcak and Greg Heinrich and Shizhe Diao and Yonggan Fu and Xin Dong and Saurav Muralidharan and Yingyan Celine Lin and Pavlo Molchanov},
  year          = {2025},
  eprint        = {2506.02153},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2506.02153}
}

@article{10.1145/3768165,
  author     = {Wang, Fali and Zhang, Zhiwei and Zhang, Xianren and Wu, Zongyu and Mo, TzuHao and Lu, Qiuhao and Wang, Wanjing and Li, Rui and Xu, Junjie and Tang, Xianfeng and He, Qi and Ma, Yao and Huang, Ming and Wang, Suhang},
  title      = {A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness},
  year       = {2025},
  issue_date = {December 2025},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {16},
  number     = {6},
  issn       = {2157-6904},
  url        = {https://doi.org/10.1145/3768165},
  doi        = {10.1145/3768165},
  abstract   = {Large language models (LLMs) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use, which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs' challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely; thus, to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected SLM models and related methods on GitHub: .},
  journal    = {ACM Trans. Intell. Syst. Technol.},
  month      = nov,
  articleno  = {145},
  numpages   = {87},
  keywords   = {Small Language Models, On-Device LLMs, Domain-specific Models, Trustworthiness}
}

@misc{subramanian2025smalllanguagemodelsslms,
  title         = {Small Language Models (SLMs) Can Still Pack a Punch: A survey},
  author        = {Shreyas Subramanian and Vikram Elango and Mecit Gungor},
  year          = {2025},
  eprint        = {2501.05465},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2501.05465}
}