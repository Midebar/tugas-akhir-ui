%
% Halaman Abstract
%
% @author  Andreas Febrian
% @version 2.2.0
% @edit by Ichlasul Affan
%

\chapter*{ABSTRACT}
\singlespacing

\noindent \begin{tabular}{l l p{11.0cm}}
	\ifx\blank\npmDua
	Name                     & :  & \penulisSatu                     \\
	Study Program            & :  & \studyProgramSatu                \\
	\else
	Writer 1 / Study Program & :  & \penulisSatu~/ \studyProgramSatu \\
	Writer 2 / Study Program & :  & \penulisDua~/ \studyProgramDua   \\
	\fi
	\ifx\blank\npmTiga\else
	Writer 3 / Study Program & :  & \penulisTiga~/ \studyProgramTiga \\
	\fi
	Title                    & :  & \judulInggris                    \\
	Counselor                & :  & \pembimbingSatu                  \\
	\ifx\blank\pembimbingDua
	\else
	\                        & \  & \pembimbingDua                   \\
	\fi
	\ifx\blank\pembimbingTiga
	\else
	\                        & \  & \pembimbingTiga                  \\
	\fi
\end{tabular} \\

\vspace*{0.5cm}

\noindent The landscape of artificial intelligence (AI) has recently undergone a fundamental shift.
The industry focus, previously fixated on the race to upscale language models (\textit{Large Language Models} - LLMs), is now shifting towards efficiency and accessibility through \textit{Small Language Models} (SLMs). With parameters under 10 billion, these models promise the democratization of technology: enabling advanced AI to run on resource-constrained devices, a vital advantage for developing countries like Indonesia. However, this downsizing sparks a critical question: does their cognitive reasoning capacity also shrink, especially in complex logical tasks where small models are prone to hallucinations or taking reasoning shortcuts (\textit{greedy reasoning})?

This research addresses this challenge by testing the effectiveness of the \textit{Neuro-Symbolic} approach, a hybrid method that uses the linguistic flexibility of neural models with the mathematical precision of symbolic logic, to strengthen SLM deduction capabilities on Indonesian datasets.
This study applies the \textit{Translation-Decomposition-Search-Resolution} framework to three categories of \textit{open-weight low-resource} models (7B-9B, 4-bit quantization): Qwen2.5-7B (Global), SEA-LION-v3-8B (Regional), and SahabatAI-v1-8B (National). Through a series of experiments using the ProntoQA logic dataset adapted into Indonesian, performance differences between conventional methods and the \textit{Neuro-Symbolic} approach were revealed.

Empirical findings show an anomaly: the global model Qwen2.5, which dominated in implicit reasoning (81.00\%), experienced catastrophic failure (dropping to 14.00\%) when forced to adhere to strict syntactic constraints within the symbolic \textit{pipeline}. Conversely, the regional model SEA-LION v3 recorded the highest performance (81.60\%) within this framework.
This proves that language alignment (\textit{regional alignment}) is a determining factor in overcoming semantic translation barriers (\textit{Translation Bottleneck}).
This study concludes that for applications demanding high logical reliability on limited infrastructure, integrating regional SLMs with symbolic \textit{solvers} offers a far more robust solution than merely relying on the model's probabilistic intuition alone. \\

\vspace*{0.2cm}

\noindent Key words: \\ Open-weight LLM, logical reasoning, ProntoQA, resolution \\

\setstretch{1.4}
\newpage
