%
% Halaman Abstract
%
% @author  Andreas Febrian
% @version 2.2.0
% @edit by Ichlasul Affan
%

\chapter*{ABSTRACT}
\singlespacing

\noindent \begin{tabular}{l l p{11.0cm}}
	\ifx\blank\npmDua
	Name                     & :  & \penulisSatu                     \\
	Study Program            & :  & \studyProgramSatu                \\
	\else
	Writer 1 / Study Program & :  & \penulisSatu~/ \studyProgramSatu \\
	Writer 2 / Study Program & :  & \penulisDua~/ \studyProgramDua   \\
	\fi
	\ifx\blank\npmTiga\else
	Writer 3 / Study Program & :  & \penulisTiga~/ \studyProgramTiga \\
	\fi
	Title                    & :  & \judulInggris                    \\
	Counselor                & :  & \pembimbingSatu                  \\
	\ifx\blank\pembimbingDua
	\else
	\                        & \  & \pembimbingDua                   \\
	\fi
	\ifx\blank\pembimbingTiga
	\else
	\                        & \  & \pembimbingTiga                  \\
	\fi
\end{tabular} \\

\vspace*{0.5cm}

\noindent Artificial intelligence (AI) is undergoing a major transformation. An industry that once focused on building increasingly large language models (Large Language Models, LLMs) is now shifting toward more efficient and affordable approaches through Small Language Models (SLMs). Models with fewer than 10 billion parameters open up significant opportunities, enabling advanced AI technologies to operate even on devices with limited computational resourcesâ€”an aspect that is particularly important for developing countries such as Indonesia. However, this reduction in model size introduces a critical challenge: does the ability for logical reasoning also diminish, especially when handling complex logical tasks where smaller models are prone to producing inaccurate results? \\

This study addresses this challenge by examining how a Neuro-Symbolic approach, a hybrid method that combines the flexibility of neural models with the precision of symbolic logic, can enhance the reasoning capabilities of SLMs on Indonesian-language data. The research implements a complete pipeline consisting of: translating questions into formal logic (Translation to First-Order Logic), converting them into conjunctive normal form (Decomposition to Conjunctive Normal Form), searching for complementing clauses (Search for Complementing Clause), and finally applying resolution using proof by contradiction (Resolution with Proof by Contradiction). The experiments involve three small open-weight models (7B-9B parameters, 4-bit quantized): Qwen2.5-7B (a multilingual foundation model), SEA-LION-v3-8B (a language-adaptive pre-trained model), and SahabatAI-v1-8B (a localized pre-trained model). This study uses the ProntoQA logical reasoning dataset adapted into Indonesian. The research compares how well each model performs on reasoning tasks with and without the Neuro-Symbolic framework. \\

The experimental results reveal notable findings. The multilingual foundation model Qwen2.5, which previously excelled at implicit reasoning under Naive Prompting with an accuracy of 81.00\%, experienced a drastic decline to 14.00\% when required to adhere to the strict syntactic rules of the symbolic pipeline due to parsability errors. In contrast, the language-adaptive pre-trained model SEA-LION-v3 achieved the best performance among the three models in this study, reaching an accuracy of 81.60\% within the Neuro-Symbolic framework. These results indicate that linguistic alignment in language-adaptive pre-trained models plays a crucial role in overcoming semantic translation barriers. Based on these findings, the study recommends that for logical reasoning in resource-constrained LLM settings, employing language-adaptive SLMs in combination with a Neuro-Symbolic framework provides a more reliable solution than relying solely on the model's probabilistic intuition.

\vspace*{0.2cm}

\noindent Key words: \\ Open-weight SLM, logical reasoning, ProntoQA bahasa Indonesia, resolution \\

\setstretch{1.4}
\newpage