%-----------------------------------------------------------------------------%
\chapter{\babDua}
\label{bab:2}
%-----------------------------------------------------------------------------%

Bab ini membangun kerangka teoretis yang mendasari analisis kemampuan \textit{Large Language Models} (LLM) dalam melakukan penalaran logika. Pembahasan mencakup prinsip formal dari \textit{First-Order Logic} (FOL), prosedur konversi ke \textit{Conjunctive Normal Form} (CNF), serta algoritma Resolusi yang menjadi mesin utama dalam \textit{framework} Aristotle.

%-----------------------------------------------------------------------------%
\section{\textit{First-Order Logic} (FOL)}
\label{sec:fol}
%-----------------------------------------------------------------------------%
First-Order Logic (FOL), atau Kalkulus Predikat, adalah logika formal yang memperluas logika proposisi dengan memperkenalkan variabel, fungsi, dan kuantor untuk merepresentasikan objek dan relasi di dunia nyata.

\subsection{Sintaks: Alfabet dan Aturan Pembentukan}
Sintaks FOL dibangun dari komponen-komponen berikut menurut \cite{brachman2004knowledge}: 
\begin{itemize}
	\item Simbol Logis:
	      \begin{itemize}
		      \item Konektif: $\neg$ (Negasi), $\land$ (Konjungsi), $\lor$ (Disjungsi), $\rightarrow$ (Implikasi), $\leftrightarrow$ (Bikondisional).
		      \item Kuantor: $\forall$ (Universal), $\exists$ (Eksistensial).
		      \item Variabel: $x, y, z, \dots$
	      \end{itemize}
	\item Simbol Non-Logis:
	      \begin{itemize}
		      \item Konstanta: Simbol yang merepresentasikan objek spesifik (misalnya, "Alice", "42").
		      \item Fungsi ($f$($x_1$, $\dots$, $x_n$)): Memetakan objek ke objek lain. Contoh: $AyahDari$($Budi$).
		      \item Predikat ($P$($x_1$, $\dots$, $x_n$)): Fungsi yang memetakan tuple objek ke nilai kebenaran (True/False). Contoh: $Ayah$($Budi, True$).
	      \end{itemize}
\end{itemize}

\subsection{Semantik: Interpretasi dan Kebenaran}
Kebenaran sebuah kalimat FOL ditentukan oleh sebuah Interpretasi ($\mathcal{I}$) atas Domain ($\mathcal{D}$) yang tidak kosong. Interpretasi memetakan:
\begin{itemize}
	\item \textbf{Konstanta:} Setiap simbol konstanta $c$ dipetakan ke elemen spesifik di $\mathcal{D}$.
	\item \textbf{Predikat:} Setiap simbol predikat $n$-ary dipetakan ke himpunan relasi $n$-ary di $\mathcal{D}$ (objek-objek mana yang memiliki sifat tersebut).
	\item \textbf{Fungsi:} Setiap simbol fungsi $f$ dipetakan ke operasi konkret pada $\mathcal{D}$. Misalnya, jika domainnya adalah bilangan bulat, simbol fungsi $plus(x, y)$ dapat diinterpretasikan sebagai operasi penjumlahan $x + y$.
\end{itemize}
Sebuah formula $\alpha$ dikatakan \textbf{Benar} di bawah interpretasi $\mathcal{I}$ (ditulis $\mathcal{I} \models \alpha$) jika fakta di dunia nyata sesuai dengan struktur kalimat tersebut.

%-----------------------------------------------------------------------------%
\section{Konsekuensi Logis \textit{(Entailment)}}
\label{sec:FOL}
%-----------------------------------------------------------------------------%
Tujuan utama sistem berbasis pengetahuan adalah menarik kesimpulan baru dari informasi yang sudah diketahui.
Dalam sistem logika, kumpulan fakta dan aturan yang kita yakini kebenarannya disebut sebagai \textbf{\textit{Knowledge Base}} (KB). 
Menurut \cite{huth-ryan-lics2}, $KB \models \alpha$ jika dan hanya jika untuk \textbf{setiap} interpretasi $\mathcal{I}$ di mana semua kalimat dalam $KB$ bernilai Benar, maka $\alpha$ juga pasti bernilai Benar. Artinya, tidak mungkin ada situasi di mana premis-premis kita benar tetapi kesimpulannya salah. Namun, memeriksa "setiap interpretasi" secara komputasi adalah mustahil karena jumlahnya bisa tak terbatas. Oleh karena itu, kita menggunakan algoritma inferensi sintaksis (seperti Resolusi) untuk membuktikan validitas tersebut secara otomatis.

%-----------------------------------------------------------------------------%
\section{Resolusi dan Unifikasi}
\label{sec:normalForms}
%-----------------------------------------------------------------------------%
Metode inferensi utama yang digunakan dalam modul \textit{resolve} Aristotle adalah \textbf{Resolusi} (\cite{brachman2004knowledge})

\subsection{Pembuktian Kontradiksi (\textit{Proof by Contradiction})}
\label{subsec:proof_by_contradiction}
Resolusi bekerja dengan prinsip \textit{Proof by Contradiction}. Untuk membuktikan bahwa $KB \models \alpha$, kita dapat menunjukkan bahwa himpunan $KB \cup \{\neg \alpha\}$ adalah \textit{Unsatisfiable} (tidak ada interpretasi yang memenuhi), maka terbukti secara logis bahwa $\alpha$ haruslah benar.
Tanda terjadinya kontradiksi adalah ketika algoritma berhasil menurunkan \textbf{Klausa Kosong} ($\square$ atau \textit{False}) (\cite{brachman2004knowledge}).

\subsection{Konversi ke \textit{Conjunctive Normal Form} (CNF)}
Agar aturan resolusi dapat diterapkan, formula logika harus diubah ke bentuk standar yang disebut \textit{Conjunctive Normal Form} (CNF). Menurut \cite{fitting-fol-atp} merinci langkah-langkah algoritma konversi ini sebagai berikut:

\begin{enumerate}
	\item Eliminasi Implikasi: Ubah $A \rightarrow B$ menjadi $\neg A \lor B$.
	\item Geser Negasi ke Dalam: Gunakan hukum De Morgan dan aturan $\neg \bracket{\forall x}{P} \equiv \bracket{\exists x} {\neg P}$.
	\item Standardisasi Variabel: Ubah nama variabel agar unik untuk setiap kuantor (misal: $\bracket{\forall x} {\bracket{P}{(x)}} \lor \bracket{\forall x} {\bracket{Q}{(x)}}$ menjadi $\bracket{\forall x} {\bracket{P}{(x)}} \lor \bracket{\forall y} {\bracket{Q}{(y)}}$).
	\item Prenex Normal Form: Pindahkan semua kuantor ke depan formula.
	\item Skolemisasi: Menghilangkan kuantor eksistensial ($\exists$). Variabel $y$ yang terikat oleh $\exists$ diganti dengan Fungsi Skolem ($\bracket{f}{(x)}$) yang bergantung pada variabel universal sebelumnya. Contoh: $\bracket{\forall x \exists y} {(\bracket{Parent}{(x, y)})}$ menjadi $\bracket{\forall x} {(\bracket{Parent}{(x, \bracket{f}{(x)})})}$.
	\item Distribusi \& CNF: Gunakan aturan distributif untuk mendapatkan bentuk konjungsi dari klausa (AND of ORs).
\end{enumerate}

\subsection{Aturan Resolusi dan Unifikasi}
Aturan resolusi untuk logika predikat adalah:
$$\frac{C_1 \lor L_1, \quad C_2 \lor L_2}{(C_1 \lor C_2)\theta}$$
Di mana $L_1$ dan $L_2$ adalah literal yang saling berlawanan (misal $\bracket{P}{(x)}$ dan $\neg \bracket{P}{(a)}$). Agar mereka bisa saling menghilangkan (\textit{cancel out}), argumen di dalamnya harus disamakan terlebih dahulu.

Proses penyamaan argumen ini disebut \textbf{Unifikasi}. Unifikasi mencari substitusi $\theta$ (disebut \textit{Most General Unifier} / MGU) yang membuat dua atom menjadi identik secara sintaksis.
Contoh: Unifikasi $\bracket{P}{(x)}$ dan $\bracket{P}{(Alex)}$ menghasilkan $\theta = \{x$ / $Alex\}$. Tanpa unifikasi, resolusi pada FOL tidak mungkin dilakukan karena variabel pada premis
yang berbeda harus diselaraskan.

%-----------------------------------------------------------------------------%
\section{\textit{Language Models} (LM)}
\label{sec:llm_general}
%-----------------------------------------------------------------------------%
Bagian ini membahas teknologi dasar yang memungkinkan implementasi \textit{Logical Translator} dalam penelitian ini.

\subsection{Arsitektur Transformer}
\textit{Large Language Models} (LLM) modern seperti Qwen, Llama, dan GPT dibangun di atas arsitektur \textbf{Transformer} yang diperkenalkan oleh \cite{NIPS2017_3f5ee243}. Inovasi utama arsitektur ini adalah mekanisme \textit{Self-Attention}, yang memungkinkan model untuk memproses hubungan antar kata dalam sebuah kalimat secara paralel tanpa bergantung pada urutan sekuensial seperti pada RNN (\textit{Recurrent Neural Networks}). Hal ini memungkinkan model menangkap ketergantungan jarak jauh (\textit{long-range dependencies}) yang penting untuk memahami konteks logika yang kompleks.

\subsection{Pre-training dan Fine-tuning}
LLM dilatih melalui dua tahap utama:
\begin{enumerate}
	\item \textbf{\textit{Pre-training}:} Model dilatih pada korpus teks yang sangat besar (triliunan token) dengan tujuan memprediksi kata berikutnya (\textit{next-token prediction}) (\cite{NEURIPS2020_1457c0d6}). Pada tahap ini, model mempelajari struktur bahasa, fakta dunia nyata, dan pola penalaran dasar secara implisit.
	\item \textbf{\textit{Fine-tuning} / \textit{Instruction Tuning}:} Model disesuaikan lebih lanjut dengan dataset instruksi-jawaban agar mampu mengikuti perintah pengguna, seperti "Terjemahkan kalimat ini ke \textit{First Order Logic}" (\cite{NEURIPS2022_b1efde53}). Model yang digunakan dalam penelitian ini (Instruct versions) telah melalui tahap ini.
\end{enumerate}

\subsection{Inferensi dan Kuantisasi}
Proses penggunaan model yang sudah dilatih untuk menghasilkan teks disebut \textbf{Inferensi}.
\begin{itemize}
	\item \textbf{Decoding Strategies:} Karena LLM memprediksi probabilitas kata berikutnya, cara kita memilih kata tersebut mempengaruhi hasil.
	      \begin{itemize}
		      \item \textit{Greedy Decoding:} Memilih token dengan probabilitas tertinggi di setiap langkah (\texttt{temperature=0}). Metode ini deterministik dan sangat cocok untuk tugas logika yang membutuhkan kepastian sintaks.
		      \item \textit{Sampling:} Memilih token secara acak berdasarkan distribusi probabilitas (\texttt{temperature>0.7}), cocok untuk tugas kreatif namun buruk untuk logika.
	      \end{itemize}
	\item \textbf{Kuantisasi (GGUF):} Menjalankan LLM membutuhkan memori (VRAM) yang besar. Untuk mengatasi ini pada perangkat keras terbatas, digunakan teknik kuantisasi, yaitu mengurangi presisi bobot model dari 16-bit (\textit{half precision}) menjadi 4-bit atau 8-bit bilangan bulat (\cite{10.5555/3600270.3602468}).
\end{itemize}

\subsection{Definisi dan Karakteristik Small Language Models (SLM)}
Dalam perkembangannya, paradigma model bahasa mulai bergeser dari "semakin besar semakin baik" menjadi efisiensi. \textit{Small Language Models} (SLM) didefinisikan secara longgar dalam literatur sebagai model dengan jumlah parameter berkisar antara beberapa juta hingga batas atas sekitar 7 hingga 10 miliar parameter (\cite{10.1145/3768165, subramanian2025smalllanguagemodelsslms}).

Karakteristik utama SLM meliputi:
\begin{itemize}
	\item \textbf{Efisiensi Parameter:} SLM mencapai performa kompetitif dengan jumlah parameter yang jauh lebih sedikit (10-100x lebih kecil dari LLM), memungkinkan inferensi yang lebih cepat dan penggunaan memori yang lebih rendah.
	\item \textbf{(\textit{Edge Deployability}):} Karena ukurannya yang ringan, SLM dapat dijalankan secara lokal pada perangkat konsumen standar, seperti laptop, bahkan ponsel, tanpa memerlukan kluster GPU \textit{data center}. Hal ini memberikan keuntungan dalam privasi data dan kedaulatan informasi (\cite{belcak2025smalllanguagemodelsfuture}).
	\item \textbf{Teknik Kompresi:} Performa tinggi SLM sering kali dicapai melalui teknik \textit{Knowledge Distillation} (belajar dari model guru yang lebih besar) dan pelatihan pada dataset berkualitas sangat tinggi atau \textit{textbook quality}, serta penggunaan teknik (\textit{pruning}) (\cite{10.1145/3768165}).
\end{itemize}
Model seperti Microsoft Phi-3, Mistral 7B, dan Gemma 2B adalah contoh representatif dari kelas model ini yang menunjukkan kemampuan penalaran yang menarik meskipun berukuran kecil ("Tiny but Mighty")\footnote{\url{https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/}}

%-----------------------------------------------------------------------------%
\section{Dataset ProntoQA}
\label{sec:prontoqa_theory}
%-----------------------------------------------------------------------------%
Untuk mengukur kemampuan penalaran secara objektif, penelitian ini mengadaptasi dataset ProntoQA yang dikembangkan oleh \cite{saparov2023language}.

\subsection{Generasi Data Sintetis}
ProntoQA berbeda dari dataset tanya-jawab umum karena sifatnya yang generatif dan sintetis. Data tidak diambil dari internet, melainkan dibangkitkan dari aturan logika formal yang kemudian diterjemahkan ke bahasa alami.
Proses pembuatannya adalah:
\begin{enumerate}
	\item Sistem membuat graf ontologi fiktif (misal: Setiap Wumpus adalah Jompus dan Setiap Jompus adalah Tumpus).
	\item Sistem menurunkan rantai deduksi berdasarkan graf tersebut.
	\item Simbol-simbol logika diganti dengan kata-kata fiktif atau nyata untuk membentuk kalimat bahasa alami.
\end{enumerate}

\subsection{Mengisolasi Kemampuan Penalaran}
Keunggulan utama ProntoQA adalah kemampuannya mengisolasi kemampuan deduksi dari pengetahuan dunia (\textit{world knowledge}).
Menggunakan istilah fiktif (seperti "Wumpus" atau "Tumpus") mencegah model menjawab benar hanya karena hafal fakta (misalnya "Burung bisa terbang"). Model dipaksa untuk melakukan penalaran ("Jika Wumpus adalah Burung, maka Wumpus bisa terbang") berdasarkan aturan yang diberikan di dalam \textit{prompt}, bukan dari memori pelatihannya.
Dataset ini juga memungkinkan kontrol terhadap "kedalaman penalaran" (\textit{reasoning hops}), yaitu berapa langkah logika yang diperlukan untuk mencapai kesimpulan (misal: 1-hop, 3-hop, hingga 5-hop).